batch_size: 32
optimizer_choice: adam
lr: 0.001
num_epochs: 20
weight_decay: 1e-6
eps: 1e-8
betas: [0.9,0.999]
clipping_gradient: null
clip_value: 1.0
gradient_interval: 5
factor: 0.5
patience: 7
minlr: 1e-8
early_stopping: 20
ema_decay: 0.01
num_sample: 1000
size_limit: 120
train_ratio: 0.7
cv: True
nfold: 5
val_interval: 2
early_stop: 10
upper_weight: 0.64
lower_weight: 1.0
mutual_info_weight: 0.0
frozen_core: False